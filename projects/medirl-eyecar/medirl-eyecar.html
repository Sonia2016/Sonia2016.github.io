
<!DOCTYPE html>
<html lang="en-US">
  

  <head>

    
    <meta charset="UTF-8">

<title>MEDIRL-EyeCar | Project</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="MEDIRL-EyeCar" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning" />
<meta property="og:description" content="MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning" />
<meta property="og:site_name" content="MEDIRL-EyeCar-web" />
<script type="application/ld+json">
{"description":"MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning","@type":"WebSite","headline":"robosuite","url":"https://soniabaee.github.io/projects/medirl-eyecar.html","name":"MEDIRL-EyeCar-web","@context":"https://schema.org"}</script>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="https://robosuite.ai/assets/css/style.css?v=ac6bb3bc266ae2f7e36177a4698a620c389ed387">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-fork-ribbon-css/0.2.3/gh-fork-ribbon.min.css" />
    <link rel="shortcut icon" type="image/x-icon" href="uva.ico" />
  </head>
  
  <body>

    <!-- <a class="github-fork-ribbon" href="https://github.com/soniabaee/MEDIRL-EyeCar" data-ribbon="Fork me on GitHub" title="Fork me on GitHub">Fork me on GitHub</a> -->

    <header class="page-header" role="banner">
      <h1 class="project-name">MEDIRL</h1>
      <h1 class="project-subtitle"></h1>
      <h2 class="project-tagline">Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning</h2>
      <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Baee_MEDIRL_Predicting_the_Visual_Attention_of_Drivers_via_Maximum_Entropy_ICCV_2021_paper.pdf" class="btn" target="_blank"> pdf </a>
      <a href="https://github.com/soniabaee/MEDIRL-EyeCar" class="btn" target="_blank">Github</a>
      <a href="https://arxiv.org/abs/1912.07773" class="btn" target="_blank">arxiv</a>
      <a href="https://github.com/soniabaee/MEDIRL-EyeCar/tree/master/EyeCar" class="btn" target="_blank">EyeCar</a>

    </header>

    <main id="content" class="main-content" role="main">
      <p><img src="MEDIRL_Schematic.jpg" alt="MEDIRL" /></p>

<p>Inspired by human visual attention, we propose a novel inverse reinforcement learning formulation using Maximum Entropy Deep Inverse Reinforcement Learning (<strong>MEDIRL</strong> ) for predicting the visual attention of drivers in accident-prone situations. MEDIRL predicts fixation locations that lead to maximal rewards by learning a task-sensitive reward function from eye fixation patterns recorded from attentive drivers. Additionally, we introduce EyeCar, a new driver attention dataset in accident-prone situations. We conduct comprehensive experiments to evaluate our proposed model on three common benchmarks: (DR(eye)VE, BDD-A, DADA-2000), and our EyeCar dataset. Results indicate that MEDIRL outperforms existing models for predicting attention and achieves state-of-the-art performance. We present extensive ablation studies to provide more insights into different features of our proposed model. </p>

<hr class="hr-round">
<h2 id="project-overview"><strong>Project Overview</strong></h2>
<p><strong><a href="https://github.com/soniabaee/eyeCar/tree/master/EyeCar">EyeCar Dataset</strong>:</a> We select 21 front-view videos that were captured in various traffic, weather, and day light conditions. Each video is 30sec in length and contains typical driving tasks (e.g., lane-keeping, merging-in, and braking) ending to rear-end collisions. Note that all the conditions were counterbalanced among all the participants. Moreover, EyeCar provides information about the speed and GPS of the ego-vehicle. In addition, each video frame comprises 4.6 vehicles on average, making EyeCar driving scenes more complex than other visual attention datasets. The <a href="https://github.com/soniabaee/eyeCar/tree/master/EyeCar">EyeCar</a> dataset contains 3.5h of gaze behavior (aggregated and raw) from the 20 participants, as well as more than 315,000 rear-end collisions video frames. In EyeCar dataset, we account for the sequence of eye fixations, and thus we emphasize on attention shift to the salient regions in a complex driving scene. EyeCar also provides a rich set of annotations(e.g., scene tagging, object bounding, lane marking, etc.). Compared to prior datasets, EyeCar is the only dataset captured from a point-of-view (POV) perspective, involving collisions, and including metadata for both speedand GPS. EyeCar also has the largest average number of vehicles per scene, and gaze data for 20 participants.</p>
<p style="text-align: center;"><img src="eyeCar.png" alt="EyeCar dataset" /></p>
<br>
<!-- <hr class="hr-round">

<p><strong>MEDIRL - Module 3. The attention module</strong>: Drivers pay attention to the task-related regions of the scene to filter out irrelevant information and ultimately make optimal decisions. Drivers do this with a sequence of eye fixations. To learn this process in various driving tasks ending in rear-end collisions, we cast it as a maximum inverse reinforcement learning approach.</p>
<p><img src="attention.png" alt="attention module" /></p>
<br> -->
<br> 
<hr class="hr-round">
<p><strong>Some results: </strong>
</p>
<p><img src="Results_reward map.png" alt="result" /></p>


<hr class="hr-round">
<h1 id="team">Team</h1>

<div class="col-lg-12 clearfix">
  <div class="col-xs-2 authornames">
    <a href="http://soniabaee.com/" align="middle" target="_blank">
      <img src="team/sonia_baee.jpeg" class="img-circle img-responsive" width="150" height="150"/>
      Sonia Baee
    </a>
  </div>
  <div class="col-xs-2 authornames">
    <a href="http://www.cs.virginia.edu/~ep2ca/" align="middle" target="_blank">
      <img src="team/erfan_pak.png" class="img-circle img-responsive" width="150" height="150"/>
      Erfan Pakdamanian
    </a>
  </div>
  <div class="col-xs-2 authornames">
    <a href="http://www.sys.virginia.edu/inki-kim.html" align="middle" target="_blank">
      <img src="team/inki_kim.png" class="img-circle img-responsive" width="150" height="20"/>
      Inki Kim
    </a>
  </div>
  <div class="col-xs-2 authornames">
    <a href="http://www.cs.virginia.edu/~lufeng/" align="middle" target="_blank">
      <img src="team/lu_feng.png" class="img-circle img-responsive" />
      Lu Feng
    </a>
  </div>
  <div class="col-xs-2 authornames">
    <a href="https://www.vicenteordonez.com" align="middle" target="_blank">
      <img src="team/vicente_ordonez.jpeg" class="img-circle img-responsive" />
      Vicente Ordonez Roman
    </a>
  </div>
  <div class="col-xs-2 authornames">
    <a href="https://faculty.virginia.edu/S2HeLab/index.php" align="middle" target="_blank">
      <img src="team/laura_barnes.jpeg" class="img-circle img-responsive" />
      Laura Barnes
    </a>
  </div>
</div>

<h1 id="citation">Citation</h1>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">baee2019eyecar</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{EyeCar: Modeling the Visual Attention Allocation of Drivers in Semi-Autonomous Vehicles}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Baee, Sonia and Pakdamanian, Erfan and Roman, Vicente Ordonez and Kim, Inki and Feng, Lu and Barnes, Laura}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:1912.07773}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2019}</span>
<span class="p">}</span>
</code></pre></div></div>


      <footer class="site-footer">
        <span class="site-footer-owner">
            &copy; Copyright University of Virginia 2020, Web template from <a href="https://robosuite.ai/">robosuite</a> 
        </span>
      </footer>
    </main>

 <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
    
  </body>
</html>
